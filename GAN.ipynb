{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z,derivative=False):\n",
    "    m=np.shape(z)[0];\n",
    "    if derivative:\n",
    "        sig_derivative=np.exp(-z)/np.square(np.ones(m)+np.exp(-z));\n",
    "        return sig_derivative;\n",
    "    else:\n",
    "        \n",
    "        sig=np.reciprocal((np.ones(m))+np.exp(-z));\n",
    "    return sig,m;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(data_set,generator_output):\n",
    "    m=len(generator_output);\n",
    "    d_loss=log(data_set)+log(np.ones(m)-generator_output);\n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generator_output):\n",
    "    m=len(generator_output);\n",
    "    g_loss=log(np.ones(m)-generator_output);\n",
    "    return g_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, size, seed=42):\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        # biases are initialized randomly\n",
    "        self.biases = [np.random.rand(m, 1) for m in self.size[1:]]\n",
    "        self.weights=[np.random.rand(self.size[i],self.size[i-1])for i in range(1,len(self.size))]\n",
    "        \n",
    "    def __init__(obj,size,seed=42):\n",
    "        obj.seed=seed\n",
    "        np.random.seed(obj.seed)\n",
    "        obj.size=size\n",
    "        obj.biases=[np.random.rand(m,1) for m in obj.size[1:]]\n",
    "        obj.weights=[np.random.rand(obj.size[i],obj.size[i-1])for i in range(1,len(self.size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_generator(obj,X):\n",
    "    batch_size=(X.shape[0])/10;\n",
    "    columns=(X.shape[1]);\n",
    "    obj.z=[np.random.rand(batch_size,columns)];   \n",
    "    return obj.z;#obj is for generator data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input):\n",
    "    a = input\n",
    "    pre_activations = []\n",
    "    activations = [a]\n",
    "       \n",
    "    for w, b in zip(self.weights, self.biases):# ANS:-packs self.weights and self.biases into a single entity\n",
    "        z = np.dot(w, a) + b  # gives the matrix multiplication of theta and feature matrices and add the bias term to give hypotheses for each row of features\n",
    "        a  = activation(z)   #gives the probability\n",
    "        pre_activations.append(z)  \n",
    "        activations.append(a)\n",
    "    return a, pre_activations, activations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(self,pre_activations,y_true,y_pred):\n",
    "    length=len(self.size)-1;\n",
    "    delta=[0]*length;\n",
    "    delta_b=[0]*length;\n",
    "    delta[length-1]=y_pred-y_true;\n",
    "    delta_new1=delta[length-1];\n",
    "    delta_new=delta[length-1];\n",
    "    for i in range(length-2,-1,-1):\n",
    "        delta[i]=np.multiply(((np.transpose(self.weights[i+1]))*delta_new),activation(pre_activations[i+1],derivative=True));\n",
    "        delta_b[i]=np.multiply(((np.transpose(self.biases[i+1]))*delta_new1),activation(pre_activations[i+1],derivative=True));\n",
    "        delta_new=delta[i];\n",
    "        delta_new1=delta_b[i];\n",
    "    return delta,delta_b;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(self,delta,pre_activations,activations):\n",
    "    length=len(delta);\n",
    "    dW=length*[0];\n",
    "    db=length*[0];\n",
    "    for i in range(length-1,-1,-1):\n",
    "        dW[i]=dW[i]+delta[i]*numpy.transpose(activations[i+1]);\n",
    "        db[i]=db[i]+delta_b[i]*numpy.transpose(activations[i+1]);\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(self,epochs,train,test):\n",
    "    plt.subplot(211)\n",
    "    plt.title('Training Cost (loss)')\n",
    "    plt.plot(range(epochs),train)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.title('Test Cost (loss)')\n",
    "    plt.plot(range(epochs),test)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training for the discriminator on the mnist data set keeping the generator untrainable\n",
    "def train_discriminator_on_real_only(self, X, y, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10):\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X.T, y.T, test_size=validation_split, )\n",
    "        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T\n",
    "\n",
    "        epoch_iterator = range(epochs) #batch_size= no. of training examples in a batch\n",
    "                                       #batch=subset of training examples from whole data set\n",
    "                 \n",
    "        m=x_train.shape[0];\n",
    "                #epoch=when whole data set gets complete\n",
    "        for e in epoch_iterator:\n",
    "            if x_train.shape[1] % batch_size == 0:\n",
    "                n_batches = int(x_train.shape[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(x_train.shape[1] / batch_size ) - 1\n",
    "\n",
    "            x_train, y_train = shuffle(x_train.T, y_train.T)  #shuffles the data set\n",
    "            x_train, y_train = x_train.T, y_train.T\n",
    "\n",
    "            batches_x = [x_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)] #grouping all the batches\n",
    "            batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)] \n",
    "\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
    "                batch_y_pred, pre_activations, activations = self.forward(batch_x)\n",
    "                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n",
    "                dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
    "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
    "                    dw_per_epoch[i] += dw_i / batch_size\n",
    "                    db_per_epoch[i] += db_i / batch_size\n",
    "\n",
    "                batch_y_train_pred = self.predict(batch_x)\n",
    "\n",
    "                train_loss = cost_function(batch_y, batch_y_train_pred)\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                batch_y_test_pred = self.predict(x_test)\n",
    "\n",
    "                test_loss = cost_function(y_test, batch_y_test_pred)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "                \n",
    "                \n",
    "                #weight update\n",
    "\n",
    "                #ANS:-\n",
    "                #the enumerate function adds index to the iterable entities beginning from 'start'\n",
    "                #and returns it as the enumerate object\n",
    "\n",
    "                for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                    self.weights=self.weights-(learning_rate/m)*dW;\n",
    "                    self.biases=self.biases-(learning_rate/m)*db;\n",
    "                history_train_losses.append(np.mean(train_losses))\n",
    "                history_train_accuracies.append(np.mean(train_accuracies))\n",
    "\n",
    "                history_test_losses.append(np.mean(test_losses))\n",
    "                history_test_accuracies.append(np.mean(test_accuracies))\n",
    "\n",
    "\n",
    "                if e % print_every == 0:\n",
    "                    print('Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} '.format(\n",
    "                        e, epochs, np.round(np.mean(train_losses), 3), np.round(np.mean(train_accuracies), 3),\n",
    "                        np.round(np.mean(test_losses), 3),  np.round(np.mean(test_accuracies), 3)))\n",
    "\n",
    "        self.plot_loss(epochs,train_loss,test_loss)\n",
    "\n",
    "        history = {'epochs': epochs,\n",
    "                   'train_loss': history_train_losses,\n",
    "                   'train_acc': history_train_accuracies,\n",
    "                   'test_loss': history_test_losses,\n",
    "                   'test_acc': history_test_accuracies\n",
    "                   }\n",
    "        return history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting the generator_data for the discriminator\n",
    "def generator_input(obj, X, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10):\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "        \n",
    "\n",
    "        epoch_iterator = range(epochs) #batch_size= no. of training examples in a batch\n",
    "                                       #batch=subset of training examples from whole data set\n",
    "                                        #epoch=when whole data set gets complete\n",
    "        for e in epoch_iterator:\n",
    "            if X[1] % batch_size == 0:\n",
    "                n_batches = int(X[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(X[1] / batch_size ) - 1\n",
    "\n",
    "            X = shuffle(X.T)  #shuffles the data set\n",
    "            X = X.T\n",
    "\n",
    "            batches_x = [X[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)] #grouping all the batches\n",
    "            #batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x in(batches_x):\n",
    "                batch_y_pred, pre_activations, activations = obj.forward(batch_x)\n",
    "        return batch_y_pred;  #returning the output by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self,a):\n",
    "    for w,b in zip(self.weights,self.biases):\n",
    "        z=np.dot(w,a)+b;\n",
    "        a=activation(z);\n",
    "    #predictions=(a>0.5).astype(int);\n",
    "    return a;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_derivative(self,obj,z):\n",
    "    Z=obj.forward(z);\n",
    "    #calculating D'(G(z))\n",
    "    y_pred,pre_activations,activations=self.forward(Z);\n",
    "    delta,delta_b=self.compute_deltas(pre_activations,0,y_pred);\n",
    "    dW_generator,dB_generator=self.backpropagate(delta,pre_activations,activations);\n",
    "    #calculating G'(z)\n",
    "    y_pred_2,pre_activations_2,activations_2=obj.forward(z);\n",
    "    delta_2,delta_b_2=obj.compute_deltas(pre_activations,0,0);\n",
    "    dW_generator_2,dB_generator_2=obj.backpropagate(delta,pre_activations,activations);\n",
    "    \n",
    "    #calculating the final derivative\n",
    "    dw_derivative_generator=-(np.multiply(dW_generator,dW_generator_2))/(np.ones(dW_generator.shape[0])-Z);\n",
    "    db_derivative_generator=-(np.multiply(dB_generator,dB_generator_2))/(np.ones(dB_generator.shape[0])-Z);\n",
    "    \n",
    "    return dw_derivative_generator,db_derivative_generator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_derivative(self,z,y):\n",
    "    y_pred,pre_activations,activations=self.forward(z);\n",
    "    delta,delta_b=self.compute_deltas(pre_activations,y,y_pred);\n",
    "    dW_discriminator,dB_discriminator=self.backpropagate(delta,pre_activations,activations);\n",
    "    \n",
    "    return dW_discriminator/(y_pred),dB_discriminator/(y_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(X,learning_rate,epochs,batch_size):\n",
    "    X_fake=generator_input(data_for_generator(X));\n",
    "    m=len(X_fake);\n",
    "    while((predict(X_fake)>0.5)==0): #in input the care of extra layer will be taken\n",
    "        dW,dB=obj.generator_derivative(X_fake);\n",
    "        obj.weights=obj.weights+(learning_rate/m)*dW;\n",
    "        obj.biases=obj.biases+(learning_rate/m)*dB;\n",
    "        \n",
    "    return obj.weights,obj.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_discriminator(self,X,z,learning_rate):\n",
    "    discriminator_data_output=self.predict(X);\n",
    "    discriminator_generator_output=self.predict(generator_input(z));\n",
    "    while(discriminator_data_output>discriminator_generator_output):\n",
    "        dW,dB=self.discriminator_derivative+self.generator_derivaitve;\n",
    "        self.weights=self.weights-(learning_rate/m)*dW;\n",
    "        self.biases=self.weights-(learning_rate/m)*dB;\n",
    "        \n",
    "    return self.weights,self.biases\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
