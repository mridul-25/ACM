{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z,derivative=False):\n",
    "    m=np.shape(z)[0];\n",
    "    if derivative:\n",
    "        sig_derivative=np.exp(-z)/np.square(np.ones(m)+np.exp(-z));\n",
    "        return sig_derivative;\n",
    "    else:\n",
    "        \n",
    "        sig=np.reciprocal((np.ones(m))+np.exp(-z));\n",
    "    return sig,m;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01766271 0.00664806 0.00246651]\n"
     ]
    }
   ],
   "source": [
    "def cost_function(y_true,y_pred):\n",
    "    m=np.shape(y_pred)[1];\n",
    "    cost=(0.5/m)*(np.sum(np.square(y_pred-y_true)));\n",
    "    return cost;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_prime(y_true,y_pred):\n",
    "    m=np.shape(y_pred)[1];\n",
    "    cost_prime=(1/m)*\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, size, seed=42):\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        # biases are initialized randomly\n",
    "        self.biases = [np.random.rand(m, 1) for m in self.size[1:]]\n",
    "        self.weights=[np.random.rand(self.size[i],self.size[i-1])for i in range(1,len(self.size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input):\n",
    "    a = input\n",
    "    pre_activations = []\n",
    "    activations = [a]\n",
    "       \n",
    "    for w, b in zip(self.weights, self.biases):# ANS:-packs self.weights and self.biases into a single entity\n",
    "        z = np.dot(w, a) + b  # gives the matrix multiplication of theta and feature matrices and add the bias term to give hypotheses for each row of features\n",
    "        a  = activation(z)   #gives the probability\n",
    "        pre_activations.append(z)  \n",
    "        activations.append(a)\n",
    "    return a, pre_activations, activations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(self,pre_activations,y_true,y_pred):\n",
    "    length=len(self.size)-1;\n",
    "    delta=[0]*length;\n",
    "    delta_b=[0]*length;\n",
    "    delta[length-1]=y_pred-y_true;\n",
    "    delta_new1=delta[length-1];\n",
    "    delta_new=delta[length-1];\n",
    "    for i in range(length-2,-1,-1):\n",
    "        delta[i]=np.multiply(((np.transpose(self.weights[i+1]))*delta_new),activation(pre_activations[i+1],derivative=True));\n",
    "        delta_b[i]=np.multiply(((np.transpose(self.biases[i+1]))*delta_new1),activation(pre_activations[i+1],derivative=True));\n",
    "        delta_new=delta[i];\n",
    "        delta_new1=delta_b[i];\n",
    "    return delta,delta_b;\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(self,deltas,pre_activations,activations):\n",
    "    length=len(delta);\n",
    "    dW=length*[0];\n",
    "    db=length*[0];\n",
    "    for i in range(length-1,-1,-1):\n",
    "        dW[i]=dW[i]+delta[i]*numpy.transpose(activations[i+1]);\n",
    "        db[i]=db[i]+delta_b[i]*numpy.transpose(activations[i+1]);\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(self,epochs,train,test):\n",
    "    plt.subplot(211)\n",
    "    plt.title('Training Cost (loss)')\n",
    "    plt.plot(range(epochs),train)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.title('Test Cost (loss)')\n",
    "    plt.plot(range(epochs),test)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train(self, X, y, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10):\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X.T, y.T, test_size=validation_split, )\n",
    "        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T\n",
    "\n",
    "        epoch_iterator = range(epochs)\n",
    "\n",
    "        for e in epoch_iterator:\n",
    "            if x_train.shape[1] % batch_size == 0:\n",
    "                n_batches = int(x_train.shape[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(x_train.shape[1] / batch_size ) - 1\n",
    "\n",
    "            x_train, y_train = shuffle(x_train.T, y_train.T)\n",
    "            x_train, y_train = x_train.T, y_train.T\n",
    "\n",
    "            batches_x = [x_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "            batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
    "                batch_y_pred, pre_activations, activations = self.forward(batch_x)\n",
    "                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n",
    "                dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
    "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
    "                    dw_per_epoch[i] += dw_i / batch_size\n",
    "                    db_per_epoch[i] += db_i / batch_size\n",
    "\n",
    "                batch_y_train_pred = self.predict(batch_x)\n",
    "\n",
    "                train_loss = cost_function(batch_y, batch_y_train_pred)\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                batch_y_test_pred = self.predict(x_test)\n",
    "\n",
    "                test_loss = cost_function(y_test, batch_y_test_pred)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "                \n",
    "                \n",
    "                #weight update\n",
    "\n",
    "                #ANS:-\n",
    "                #the enumerate function adds index to the iterable entities beginning from 'start'\n",
    "                #and returns it as the enumerate object\n",
    "\n",
    "                for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                    self.weights=self.weights-(learning_rate/batch_size)*dW;\n",
    "                    self.biases=self.weights-(learning_rate/batch_size)*db;\n",
    "                history_train_losses.append(np.mean(train_losses))\n",
    "                history_train_accuracies.append(np.mean(train_accuracies))\n",
    "\n",
    "                history_test_losses.append(np.mean(test_losses))\n",
    "                history_test_accuracies.append(np.mean(test_accuracies))\n",
    "\n",
    "\n",
    "                if e % print_every == 0:\n",
    "                    print('Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} '.format(\n",
    "                        e, epochs, np.round(np.mean(train_losses), 3), np.round(np.mean(train_accuracies), 3),\n",
    "                        np.round(np.mean(test_losses), 3),  np.round(np.mean(test_accuracies), 3)))\n",
    "\n",
    "        self.plot_loss(epochs,train_loss,test_loss)\n",
    "\n",
    "        history = {'epochs': epochs,\n",
    "                   'train_loss': history_train_losses,\n",
    "                   'train_acc': history_train_accuracies,\n",
    "                   'test_loss': history_test_losses,\n",
    "                   'test_acc': history_test_accuracies\n",
    "                   }\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self,a):\n",
    "    for w,b in zip(self.weights,self.biases):\n",
    "        z=np.dot(w,a)+b;\n",
    "        a=activation(z);\n",
    "    predictions=(a>0.5).astype(int);\n",
    "    return predictions;\n",
    "        \n",
    "    \n",
    "           \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
